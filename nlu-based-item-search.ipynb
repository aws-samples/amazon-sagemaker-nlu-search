{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Install tqdm to have progress bar\n",
    "!pip install tqdm\n",
    "\n",
    "#install necessary pkg to make connection with elasticsearch domain\n",
    "!pip install \"elasticsearch<7.14.0\"\n",
    "!pip install requests\n",
    "!pip install requests-aws4auth\n",
    "!pip install \"sagemaker>=2.0.0<3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook in SageMaker Studio, you need to make sure `ipywidgets` is installed and restart the kernel, so please uncomment the code in the next cell, and run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# import IPython\n",
    "# import sys\n",
    "\n",
    "# !{sys.executable} -m pip install ipywidgets\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "print(f'SageMaker SDK Version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"nlu-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "es_host = outputs['esHostName']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Data Preparation\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "import json\n",
    "import tqdm\n",
    "import urllib.request\n",
    "from tqdm import notebook\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "# images_path = 'data/feidegger/fashion'\n",
    "# filename = 'metadata.json'\n",
    "\n",
    "# my_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "# os.makedirs(images_path, exist_ok=True)\n",
    "\n",
    "# def download_metadata(url):\n",
    "#     if not os.path.exists(filename):\n",
    "#         urllib.request.urlretrieve(url, filename)\n",
    "        \n",
    "# #download metadata.json to local notebook\n",
    "# download_metadata('https://raw.githubusercontent.com/zalandoresearch/feidegger/master/data/FEIDEGGER_release_1.1.json')\n",
    "\n",
    "# def generate_image_list(filename):\n",
    "#     metadata = open(filename,'r')\n",
    "#     data = json.load(metadata)\n",
    "#     url_lst = []\n",
    "#     for i in range(len(data)):\n",
    "#         url_lst.append(data[i]['url'])\n",
    "#     return url_lst\n",
    "\n",
    "\n",
    "# def download_image(url):\n",
    "#     urllib.request.urlretrieve(url, images_path + '/' + url.split(\"/\")[-1])\n",
    "                    \n",
    "# #generate image list            \n",
    "# url_lst = generate_image_list(filename)     \n",
    "\n",
    "# workers = 2 * cpu_count()\n",
    "\n",
    "# #downloading images to local disk\n",
    "# process_map(download_image, url_lst, max_workers=workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uploading dataset to S3\n",
    "\n",
    "# files_to_upload = []\n",
    "# dirName = 'data'\n",
    "# for path, subdirs, files in os.walk('./' + dirName):\n",
    "#     path = path.replace(\"\\\\\",\"/\")\n",
    "#     directory_name = path.replace('./',\"\")\n",
    "#     for file in files:\n",
    "#         files_to_upload.append({\n",
    "#             \"filename\": os.path.join(path, file),\n",
    "#             \"key\": directory_name+'/'+file\n",
    "#         })\n",
    "        \n",
    "\n",
    "# def upload_to_s3(file):\n",
    "#         my_bucket.upload_file(file['filename'], file['key'])\n",
    "        \n",
    "# #uploading images to s3\n",
    "# process_map(upload_to_s3, files_to_upload, max_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lauange Translation\n",
    "\n",
    "This dataset has product descriptions in German, So we will use Amazon Translate for English translation for each German sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"movie_data.json\") as json_file:\n",
    "    results = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define translator function\n",
    "# def translate_txt(data):\n",
    "#     results = {}\n",
    "#     results['filename'] = f's3://{bucket}/data/feidegger/fashion/' + data['url'].split(\"/\")[-1]\n",
    "#     results['descriptions'] = []\n",
    "#     translate = boto3.client(service_name='translate', use_ssl=True)\n",
    "#     for i in data['descriptions']:\n",
    "#         result = translate.translate_text(Text=str(i), \n",
    "#             SourceLanguageCode=\"de\", TargetLanguageCode=\"en\")\n",
    "#         results['descriptions'].append(result['TranslatedText'])\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using realtime traslation which will take around ~35 min. \n",
    "# workers = 1 * cpu_count()\n",
    "\n",
    "# #downloading images to local disk\n",
    "# results = process_map(translate_txt, data, max_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the translated text in json format in case you need later time\n",
    "# with open('zalando-translated-data.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save the model to disk which we will host at sagemaker\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "saved_model_dir = 'transformer'\n",
    "os.makedirs(saved_model_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\") \n",
    "\n",
    "tokenizer.save_pretrained(saved_model_dir)\n",
    "model.save_pretrained(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining default bucket for SageMaker pretrained model hosting\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip the model in tar.gz format\n",
    "!cd transformer && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the model to S3\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='sentence-transformers-model')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a PyTorchModel object. The deploy() method on the model object creates an endpoint which serves prediction requests in real-time. If the instance_type is set to a SageMaker instance type (e.g. ml.m5.large) then the model will be deployed on SageMaker. If the instance_type parameter is set to *__local__* then it will be deployed locally as a Docker container and ready for testing locally.\n",
    "\n",
    "First we need to create a [`Predictor`](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) class to accept TEXT as input and output JSON. The default behaviour is to accept a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "class StringPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(model_data = inputs, \n",
    "                             role=role, \n",
    "                             entry_point ='inference.py',\n",
    "                             source_dir = './code',\n",
    "                             py_version = 'py3', \n",
    "                             framework_version = '1.7.1',\n",
    "                             predictor_cls=StringPredictor)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.g4dn.xlarge', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name = f'nlu-search-model-{int(time.time())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace Transformers uses BERT pretrained model so it will generate 768 dimension for the given text. we will quickly validate the same in next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a quick test to make sure model is generating the embeddings\n",
    "import json\n",
    "payload = 'a halloween movie'\n",
    "features = predictor.predict(payload)\n",
    "embedding = json.loads(features)\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a KNN Index in Elasticsearch\n",
    "\n",
    "KNN for Amazon Elasticsearch Service lets you search for points in a vector space and find the \"nearest neighbors\" for those points by cosine similarity. Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "KNN cosine similarity requires Elasticsearch 7.7 or later (Default is Euclidean distance). Full documentation for the Elasticsearch feature, including descriptions of settings and statistics, is available in the Open Distro for Elasticsearch documentation. For background information about the k-nearest neighbors algorithm\n",
    "\n",
    "In this step we'll get all the translated product descriptions of *__zalandoresearch__* dataset and import those embadings into Elastichseach 7.7 domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the Elasticsearch connection\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "region = 'us-east-1' # e.g. us-east-1\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts = [{'host': es_host, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN index maping\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "           \"movie_nlu_vector\": { \n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768\n",
    "            } \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Elasticsearch index\n",
    "es.indices.create(index=\"idx_movie\",body=knn_index,ignore=400) # change the index name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to load zalando-translated-data.json un-comment and execute the following code\n",
    "# with open('zalando-translated-data.json') as json_file:\n",
    "#     results = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each product, we are concatenating all the \n",
    "# product descriptions into a single sentence,\n",
    "# so that we will have one embedding for each product\n",
    "\n",
    "# def concat_desc(results):\n",
    "#     obj = {\n",
    "#         'filename': results['filename'],\n",
    "#     }\n",
    "#     obj['descriptions'] = ' '.join(results['descriptions'])\n",
    "#     return obj\n",
    "\n",
    "# concat_results = map(concat_desc, results)\n",
    "# concat_results = list(concat_results)\n",
    "# concat_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to import the feature vectors corrosponds to each S3 URI into Elasticsearch KNN index\n",
    "# This process will take around ~2 min.\n",
    "\n",
    "def es_import(concat_result):\n",
    "    vector = json.loads(predictor.predict(concat_result['description']))\n",
    "    es.index(index='idx_movie',\n",
    "             body={\"movie_nlu_vector\": vector,\n",
    "                   \"title\": concat_result['title'],\n",
    "                   \"description\": concat_result['description']}\n",
    "            )\n",
    "        \n",
    "workers = 4 * cpu_count()\n",
    "    \n",
    "process_map(es_import, results, max_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Index Search Results\n",
    "\n",
    "In this step we will use SageMaker SDK as well as Boto3 SDK to query the Elasticsearch to retrive the nearest neighbours and retrive the relevent product images from Amazon S3 to display in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define display_image function\n",
    "# from PIL import Image\n",
    "# import io\n",
    "# def display_image(bucket, key):\n",
    "#     s3_object = my_bucket.Object(key)\n",
    "#     response = s3_object.get()\n",
    "#     file_stream = response['Body']\n",
    "#     img = Image.open(file_stream)\n",
    "#     return display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker SDK Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SageMaker SDK approach\n",
    "import json\n",
    "# payload = 'I want a dress that is yellow and flowery'\n",
    "# features = predictor.predict(payload)\n",
    "# embedding = json.loads(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ES index search\n",
    "import json\n",
    "# k = 3\n",
    "# idx_name = 'idx_zalando'\n",
    "# res = es.search(request_timeout=30, index=idx_name,\n",
    "#                 body={'size': k, \n",
    "#                       'query': {'knn': {'zalando_nlu_vector': {'vector': embedding, 'k': k}}}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the image\n",
    "\n",
    "# for x in res['hits']['hits']:\n",
    "#     key = x['_source']['image']\n",
    "#     key = key.replace(f's3://{bucket}/','')\n",
    "#     img = display_image(bucket,key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boto3 Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling SageMaker Endpoint\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "ENDPOINT_NAME = predictor.endpoint\n",
    "response = client.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='text/plain',\n",
    "                                       Body=payload)\n",
    "\n",
    "response_body = json.loads((response['Body'].read()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ES index search\n",
    "import json\n",
    "k = 3\n",
    "idx_name = 'idx_movie'\n",
    "res = es.search(request_timeout=30, index=idx_name,\n",
    "                body={'size': k, \n",
    "                      'query': {'knn': {'movie_nlu_vector': {'vector': response_body, 'k': k}}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the name\n",
    "\n",
    "for i in range(k):\n",
    "    key = res['hits']['hits'][i]['_source']['title']\n",
    "    print(key)\n",
    "    # key = key.replace(f's3://{bucket}/','')\n",
    "    # img = display_image(bucket,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Full-Text Amazon ES Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_search_body = {\n",
    "    \"_source\": {\n",
    "        \"excludes\": [ \"movie_nlu_vector\" ]\n",
    "    },\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"description\" : {\n",
    "                \"query\" : payload\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result_fuzzy = es.search(request_timeout=30, index=idx_name,\n",
    "                body=fuzzy_search_body)\n",
    "    \n",
    "for x in result_fuzzy['hits']['hits'][:3]:\n",
    "    key = x['_source']['title']\n",
    "    # key = key.replace(f's3://{bucket}/','')\n",
    "    # img = display_image(bucket,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a full-stack NLU search application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.Object(bucket, 'backend/template.yaml').upload_file('./backend/template.yaml', ExtraArgs={'ACL':'public-read'})\n",
    "\n",
    "\n",
    "sam_template_url = f'https://{bucket}.s3.amazonaws.com/backend/template.yaml'\n",
    "\n",
    "# Generate the CloudFormation Quick Create Link\n",
    "\n",
    "print(\"Click the URL below to create the backend API for NLU search:\\n\")\n",
    "print((\n",
    "    'https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create/review'\n",
    "    f'?templateURL={sam_template_url}'\n",
    "    '&stackName=nlu-search-api'\n",
    "    f'&param_BucketName={outputs[\"s3BucketTraining\"]}'\n",
    "    f'&param_DomainName={outputs[\"esDomainName\"]}'\n",
    "    f'&param_ElasticSearchURL={outputs[\"esHostName\"]}'\n",
    "    f'&param_SagemakerEndpoint={predictor.endpoint}'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working Amazon SageMaker endpoint for extracting image features and a KNN index on Elasticsearch, you are ready to build a real-world full-stack ML-powered web app. The SAM template you just created will deploy an Amazon API Gateway and AWS Lambda function. The Lambda function runs your code in response to HTTP requests that are sent to the API Gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the content of the Lambda function code.\n",
    "!pygmentize backend/lambda/app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once the CloudFormation Stack shows CREATE_COMPLETE, proceed to this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "api_endpoint = get_cfn_outputs('nlu-search-api')['TextSimilarityApi']\n",
    "\n",
    "with open('./frontend/src/config/config.json', 'w') as outfile:\n",
    "    json.dump({'apiEndpoint': api_endpoint}, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deploy frontend services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add NPM to the path so we can assemble the web frontend from our notebook code\n",
    "\n",
    "from os import environ\n",
    "\n",
    "npm_path = ':/home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin'\n",
    "\n",
    "if npm_path not in environ['PATH']:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    ADD_NPM_PATH = ADD_NPM_PATH + npm_path\n",
    "else:\n",
    "    ADD_NPM_PATH = environ['PATH']\n",
    "    \n",
    "%set_env PATH=$ADD_NPM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd ./frontend/\n",
    "\n",
    "!npm install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm run-script build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosting_bucket = f\"s3://{outputs['s3BucketHostingBucketName']}\"\n",
    "\n",
    "!aws s3 sync ./build/ $hosting_bucket --acl public-read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Browse your frontend service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Click the URL below:\\n')\n",
    "print(outputs['S3BucketSecureURL'] + '/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Website](./frontendExample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the search bar, try typing *__\"I want a summery dress that's flowery and yellow\"__*. Notice how the results are more relevant in the KNN Search method in contrast with the conventional full-text match search query. The KNN Search method uses features generated from the search text and finds similar product descriptions, which allows it to find similar products, even if the description doesn't have the exact same phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "We have used pretrained BERT model from sentence-transformers. You can fine tune BERT model based on your own usecase with your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Make sure that you stop the notebook instance, delete the Amazon SageMaker endpoint and delete the Elasticsearch domain to prevent any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "predictor.delete_endpoint()\n",
    "\n",
    "# Empty S3 Contents\n",
    "training_bucket_resource = s3_resource.Bucket(bucket)\n",
    "training_bucket_resource.objects.all().delete()\n",
    "\n",
    "hosting_bucket_resource = s3_resource.Bucket(outputs['s3BucketHostingBucketName'])\n",
    "hosting_bucket_resource.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
